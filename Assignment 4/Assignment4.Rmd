---
title: "Assignment 4"
author: "Tenzin Tashi"
date: "May 07, 2022"
---
############## Exercise 4.1 ##############

### a.Import the survey data survey.csv into R, storing rows 1 through 600 as training data and rows 601 through 750 as testing data.
```{r}
#Import survey.csv
data = read.csv("Dataset/survey.csv")
#Split data
data_train = head(data,600)
data_test = tail(data, 150)
```

### b. Build a classification tree from the training data using the “rpart” package, according to the formula “MYDEPV ~ Price + Income + Age”. Use the information gain splitting index. Which features were actually used to construct the tree? (see the “printcp” function) Plot the tree using the “rpart.plot” package.
```{r}
library(rpart)
library(rpart.plot)
decision_tree <- rpart(as.factor(MYDEPV) ~ Price + Income + Age, data = data_train, method = 'class', parms = list(split = 'information'))
printcp(decision_tree)
#Plot the tree
rpart.plot(decision_tree,extra = 106)
```  

#### Features were actually used to construct the tree: Age, Income, Price

#### There are 11 internal nodes in the tree, and the tree high is 6. 

### c. Score the model with the training data and create the model’s confusion matrix.  Which class of MYDEPV was the model better able to classify?

```{r message=FALSE}
library(caret)
Pred <- predict(decision_tree, data_train, type = 'class')
Matrix <- confusionMatrix(Pred, as.factor(data_train$MYDEPV))
Matrix
```   

#### As the missclassification rates for both classes are almost equal, one can conclude that each class was classified equally well 

#### The zero class missclassification rate: 26/(26+314) = `r 26/(26+314)`  

#### The one class missclassification rate: 19/(19+241) = `r 19/(19+241)`  


### d. Define the resubstitution error rate, and then calculate it using the confusion matrix from the previous step.  Is it a good indicator of predictive performance?  Why or why not? 


#### The resubstitution error rate is the number of incorrect classifications divided by the total number of classifications.

#### The resubstitution error rate: (19 + 26)/(19 + 26 + 314 +241) = `r (19 + 26)/(19 + 26 + 314 +241)`  

#### In that case, it is a good indicator of predictive performance because the training data is used to train the tree and the tree usually doing well on its training data.  

### e.Using the “ROCR” package, plot the receiver operating characteristic (ROC) curve. Calculate the area under the ROC curve (AUC).  Describe the usefulness of this statistic. 

```{r}
library(ROCR)
pred <- prediction(predict(decision_tree, type="prob")[,2], data_train$MYDEPV)
#Plot the ROC curve
roc <- performance(pred, "tpr", "fpr")
plot(roc, col='blue', main='ROC Analysis, using library ROCR')
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=2)
# Calculate the area under the ROC curve
auc <- performance(pred, "auc")
auc@y.values
```  
#### The value of AUC is equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one.
#### ROC analysis for the tree: 

- For your tree, the ROC curve is a non-decreasing curve. 

- For your tree, the ROC curve is in the form of two connected line segments. 

### f. Score the model with the testing data.  How accurate are the tree’s predictions? 

```{r}
Pred <- predict(decision_tree, data_test, type = 'class')
Matrix <- confusionMatrix(Pred, as.factor(data_test$MYDEPV))
Matrix
``` 

#### The zero class missclassification rate: 10/(10+76) = `r 10/(10+76)`  

#### The one class missclassification rate: 6/(6+58) = `r 6/(6+58)` 

- The model performed well for both classes

- Due to the small amount of testing data, one can conclude that each class was classified almost equally well or bad. 

### g. Repeat part (a), but set the splitting index to the Gini coefficient splitting index.  How does the new tree compare to the previous one? 

```{r}
gini_tree <- rpart(as.factor(MYDEPV) ~ Price + Income + Age, data = data_train, method = 'class', parms = list(split = 'gini'))
printcp(gini_tree)
#Plot the tree
rpart.plot(gini_tree,extra = 106)
```  

#### In this case, the same model is created regardless of our choice of splitting index. That difference/similarity is not generally the case.  

### h. Pruning is a technique that reduces the size/depth of a decision tree by removing sections with low classification power, which helps reduce overfitting and simplifies the model, reducing the computational cost.  One way to prune a tree is according to the complexity parameter associated with the smallest cross-validation error.  Prune the new tree in this way using the “prune” function.  Which features were actually used in the pruned tree?  Why were certain variables not used?  

#### Based on the results of step a, cross-validation error min when cp = 0.011538 

```{r}
pruned <- prune(decision_tree, cp=0.011538)
printcp(pruned)
rpart.plot(pruned, extra = 106)
```  

#### Features were actually used to construct the tree: Age, Income, Price

#### There are 5 internal nodes in the tree, and the tree high is 3. 

### i. Create the confusion matrix for the new model, and compare the performance of the model before and after pruning.

```{r}
Pred <- predict(pruned, data_train, type = 'class')
Matrix <- confusionMatrix(Pred, as.factor(data_train$MYDEPV))
Matrix
``` 

#### The zero class missclassification rate: 18/(18+322) = `r 18/(18+322)`  

#### The one class missclassification rate: 43/(43+217) = `r 43/(43+217)` 

#### The overall missclassification rate: (18+43)/600 = `r (18+43)/600`  

#### Overall model performance is slightly deteriorated, but essentially they are same  

############## Exercise 4.2 ##############
## *** Part I ***

###  (Naïve Bayes) In this assignment you will train a Naïve Bayes classifier on categorical data and predict individuals’ incomes.

### a. Import the nbtrain.csv file. Use the first 9010 records as training data and the remaining 1000 records as testing data.

```{r}
#Import nbtrain.csv
data = read.csv("Dataset/nbtrain.csv")
#Split training data vs testing data
library(caret)
data_train <- head(data, 9010)
data_test <- tail(data, 1000)
```   

### b. Construct the Naïve Bayes classifier from the training data, according to the formula “income ~ age + sex + educ”. To do this, use the “naiveBayes” function from the “e1071” package. Provide the model’s a priori and conditional probabilities.
```{r}
library(e1071)
NBclassfier <- naiveBayes(as.factor(income) ~ age + sex + educ, data=data_train)
NBclassfier
```  

### A-priori probabilities:  

- Income is in the range 10-50K: 0.803 

- Income is in the range 50-80K: 0.126  

- Income is in the range GT 80K: 0.072  

### Conditional probabilities 

#### Age
```{r}
NBclassfier$tables$age
``` 

#### Sex
```{r}
NBclassfier$tables$sex
```   

#### Education
```{r}
NBclassfier$tables$educ
``` 

### c. Score the model with the testing data and create the model’s confusion matrix. Also, calculate the overall, 10-50K, 50-80K, and GT 80K misclassification rates. Explain the variation in the model’s predictive power across income classes.

```{r}
testPred <- predict(NBclassfier, data_test, type="class")
message("Confusion Matrix for Test Data")
Matrix <- confusionMatrix(testPred, as.factor(data_test$income))
Matrix
```   
##### The overall misclassification rate: 1 - Accuracy = `r 1 - Matrix$overall[1]`

```{r}
library(shipunov)
Misclass(testPred, as.factor(data_test$income))
```
- The 10-50K misclassification rate: 0.8%  

- The 50-80K misclassification rate: 100%  

- The GT 80K misclassification rate: 89.3% 

In this model variation is explaeined mostly by confusion matrix

## *** Part II ***  

### a. Construct the classifier according to the formula “sex ~ age + educ + income”, and calculate the overall, female, and male misclassification rates.  Explain the misclassification rates?
```{r}
NBclassfier <- naiveBayes(as.factor(sex) ~ age + income + educ, data=data_train)
NBclassfier
testPred <- predict(NBclassfier, data_test, type="class")
Matrix <- confusionMatrix(testPred, as.factor(data_test$sex))
```    

##### The overall misclassification rate: 1 - Accuracy = `r 1 - Matrix$overall[1]`

```{r}
Misclass(testPred, as.factor(data_test$sex))
```
- The female misclassification rate: 75.2%  

- The male misclassification rate: 16.9%  

### b. Divide the training data into two partitions, according to sex, and randomly select 3500 records from each partition. Reconstruct the model from part (a) from these 7000 records. Provide the model’s a priori and conditional probabilities.

```{r message= FALSE}
library(dplyr)
#Divide the training data into two partitions
data_female = subset(data_train, data_train$sex == 'F')
data_male = subset(data_train, data_train$sex=='M')
#Randomly select 3500 records from each partition
data_female = sample_n(data_female, 3500)
data_male = sample_n(data_male, 3500)
new_data = rbind(data_male,data_female)
model <- naiveBayes(as.factor(sex) ~ age + income + educ, data=new_data)
message("Model Navie Bayes Classifier")
model
```

The a priori probabilities are equal and the conditional probabilities are very similar. 

#### c. How well does the model classify the testing data?  
```{r}
testPred <- predict(model, data_test, type="class")
Matrix <- confusionMatrix(testPred, as.factor(data_test$sex))
Matrix$table
message("Accuracy")
Matrix$overall[1]
```  

### d. Repeat step (b) 4 several times.  What effect does the random selection of records have on the model’s performance?  


1. 
```{r}
#Divide the training data into two partitions
data_female = subset(data_train, data_train$sex == 'F')
data_male = subset(data_train, data_train$sex=='M')
#Randomly select 3500 records from each partition
data_female = sample_n(data_female, 3500)
data_male = sample_n(data_male, 3500)
new_data = rbind(data_male,data_female)
model <- naiveBayes(as.factor(sex) ~ age + income + educ, data=new_data)
model
testPred <- predict(model, data_test, type="class")
Matrix <- confusionMatrix(testPred, as.factor(data_test$sex))
message("Accuracy")
Matrix$overall[1]
```  

2. 
```{r}
#Divide the training data into two partitions
data_female = subset(data_train, data_train$sex == 'F')
data_male = subset(data_train, data_train$sex=='M')
#Randomly select 3500 records from each partition
data_female = sample_n(data_female, 3500)
data_male = sample_n(data_male, 3500)
new_data = rbind(data_male,data_female)
model <- naiveBayes(as.factor(sex) ~ age + income + educ, data=new_data)
model
testPred <- predict(model, data_test, type="class")
Matrix <- confusionMatrix(testPred, as.factor(data_test$sex))
message("Accuracy")
Matrix$overall[1]
```    

3. 
```{r}
#Divide the training data into two partitions
data_female = subset(data_train, data_train$sex == 'F')
data_male = subset(data_train, data_train$sex=='M')
#Randomly select 3500 records from each partition
data_female = sample_n(data_female, 3500)
data_male = sample_n(data_male, 3500)
new_data = rbind(data_male,data_female)
model <- naiveBayes(as.factor(sex) ~ age + income + educ, data=new_data)
model
testPred <- predict(model, data_test, type="class")
Matrix <- confusionMatrix(testPred, as.factor(data_test$sex))
message("Accuracy")
Matrix$overall[1]
```    

4. 
```{r}
#Divide the training data into two partitions
data_female = subset(data_train, data_train$sex == 'F')
data_male = subset(data_train, data_train$sex=='M')
#Randomly select 3500 records from each partition
data_female = sample_n(data_female, 3500)
data_male = sample_n(data_male, 3500)
new_data = rbind(data_male,data_female)
model <- naiveBayes(as.factor(sex) ~ age + income + educ, data=new_data)
model
testPred <- predict(model, data_test, type="class")
Matrix <- confusionMatrix(testPred, as.factor(data_test$sex))
message("Accuracy")
Matrix$overall[1]
```    


### e. What conclusions can one draw from this exercise?
Conditional probabilities are very close over the entire sample. 
