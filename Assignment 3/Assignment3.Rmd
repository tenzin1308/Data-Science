---
title: "Assignment 3"
author: "Tenzin Tashi"
date: "April 28, 2022"
output:
  html_document:
    df_print: paged
---
############## Exercise 3.1 ##############

```{r include = FALSE}
library(arules)
library(arulesViz)
```

### Create a frequent item plot, and a frequent item table. 
```{r}
txn = read.transactions("Dataset/AssociationRules.csv")
#frequent item plot
itemFrequencyPlot(txn)
```
```{r}
#frequent item table
tab <- itemFrequency(txn)
head(tab)
```

#### a. Determine the most frequent item bought in the store.
```{r}
tail(sort(tab),1)
```

#### b. How many items were bought in the largest transaction?
```{r}
max(colSums(txn@data))
```

###  Mine the Association rules with a minimum Support of 1% and a minimum Confidence of 0%.
```{r}
rules = apriori(txn, parameter=list(support=0.01, confidence=0.0))
```  
#### c. How many rules appear in the data?  

Number of rules will appear in "Writing ...[... rule(s)]"

In this task, number of rules is 11524

#### d. How many rules are observed when the minimum confidence is 50%.
```{r}
rules = apriori(txn, parameter=list(support=0.01, confidence=0.5))
```
Number of rules is 1165

#### e. Explain how the specified confidence impacts the number of rules. 

The specified confidence, say 50%, reduces the number of rules by only considering the transactions that have at least a pair of items at least 50% of the time.

###  Create a scatter plot comparing the parameters support and confidence on the axis, and lift with shading.  
```{r warning=FALSE}
rules <- apriori(txn,parameter =list(supp=0.01,conf =0.0),control = list(verbose = FALSE))
plot(rules, jitter = 0)
```

#### f. Identify the positioning of the interesting rules. 

The interesting rules have high confidence and high lift, they would be located on the top left of the plot.

### Compare support and lift.   

#### g. Create a scatter plot measuring support vs. lift; record your observations. 
```{r warning=FALSE}
plot(rules, measure = c("support", "lift"), shading = "confidence", jitter = 0)
```  

#### h. Where are the rules located that would be considered interesting and useful? 

The most important rules would be located on the top left of the graph.

#### i. One downside to the Apriori algorithm, is that extraneous rules can be generated that are not particularly useful.  Identify where these rules are located on the graph.  Explain the relationship between the expected observation of these itemsets and the actual observation of the itemsets. 

The rules that are not particularly useful have high confidence but low lift. They would be considered coincidental would be on the bottom left of the graph. 

#### j. Using the interaction tool for a scatter plot, identify 3 rules that appear in at least 10% of the transactions by coincidence.
```{r warning=FALSE}
rules1 <- apriori(txn,parameter =list(supp=0.1,conf =0.0),control = list(verbose = FALSE))
plot(rules1,  jitter = 0)
```   
Identify 3 rules that appear in at least 10% of the transactions by coincidence: 

item37 -> item13  

item20 -> item13 

item3 -> item13 

###  Identify the most interesting rules by extracting the rules in which the Confidence is >0.8. Observe the output of the data table for the most interesting rules.
```{r}
subrules <- subset(rules,rules@quality$confidence>0.8)
inspect(subrules)
```

#### k. Sort the rules stating the highest lift first.  Provide the 10 rules with the lowest lift. Do they appear to be coincidental (Use lift = 2 as baseline for coincidence)?  Why or why not? 
```{r}
a <- sort(subrules, by = "lift")
inspect(a)
#Provide the 10 rules with the lowest lift
inspect(tail(a,10))
```  

Yes, they appear to be coincidental (except rule ##1). Because they have high confidence but low lift (lift <2).

### Create a Matrix-based visualization of two measures with colored squares.  The two measures should compare confidence and lift (have recorded = FALSE).  Note that 4 interesting rules stand out on the graph. 
```{r}
plot(subrules, method="matrix",shading = c("lift","confidence"), control = list(reorder = FALSE))
```

#### m. What can you infer about rules represented by a dark blue color? 

Rules in a dark (deep) blue color suggest that we are likely to see these itemsets paired together by coincidence making them interesting but not important rules 

### Extract the three rules with the highest lift.

#### n. Record the Rules.  Explain why these rules vary from the rules in Step 3. 
```{r}
subrules2 <- head(sort(rules, by="lift"), 3)
inspect(subrules2)
```

These rules vary from earlier because the associations between these items happen more than expected (high lift), but they do not occur more than 80% of the time.

#### o. Create a Graph-based visualization with items and rules as vertices
```{r}
 plot(subrules2, method="graph")
```

#### p. Based on your observations, explain how you would expect association rules to relate to order (i.e. the number of items contained in the rule).

- Support and order have a strong inverse relationship 

- These rules vary from earlier because the associations between these items happen more than expected, but they do not occur more than 80% of the time.  

### Create a training set from the first 8,000 transactions. Create a testing set from the last 2,000 transactions.  Run the algorithm on each dataset.  Compare the results.
```{r}
train_set = head(txn, 8000)
test_set = tail(txn, 2000)
rules_train = apriori(train_set, parameter = list(supp =0.01, conf = 0.8),control = list(verbose = FALSE))
inspect(rules_train)
rules_test = apriori(test_set, parameter = list(supp =0.01, conf = 0.8),control = list(verbose = FALSE))
inspect(rules_test)
```  

We see that majority of the rules that are present in the training set are also present in the hold out set with similar support and confidences.  

=> We can conclude by making a test set from hold out data that the rules generated by the algorithm are true for the population we are studying.


############## Exercise 3.2 ##############
## Gather and Prepare Data
```{r}
data = read.csv("Dataset/zeta.csv")
#Remove all  meanhouseholdincome duplicates (only females records should be in the dataset)
data = subset(data, data$sex == 'F')
#Remove the columns zcta and sex
data = subset(data, select = -c(zcta, sex))
#Remove outliers
##8 < meaneducation < 18
data = subset(data, meaneducation <18 & meaneducation >8)
##10,000 < meanhouseholdincome < 200,000
data <- subset(data, meanhouseholdincome <200000 & meanhouseholdincome >10000)
##0 < meanemployment < 3
data <- subset(data, meanemployment <3 & meanemployment >0)
##20 < meanage < 60
data <- subset(data, meanage <60 & meanage >20)
#Create a variable called log_income = log10(meanhouseholdincome)
data$log_income <- log10(data$meanhouseholdincome)
#Rename the columns
names(data)[names(data)=="meanage"] <- "age"
names(data)[names(data)=="meaneducation"] <- "education"
names(data)[names(data)=="meanemployment"] <- "employment"
```

## Linear Regression Analysis   

### a. Create a scatter plot showing the effect age has on log_income and paste it here.  Do you see any linear relationship between the two variables?
```{r message=FALSE}
library(ggplot2)
ggplot(data,aes(x= age, y=log_income)) +geom_point(alpha=0.2) +labs(x="Age",y="Log_Income",title="Scaterrplot Log_Income vs Age")
```  
```{r}
#correlation
cor(data$age, data$log_income)
```  

From the scatter plot We can see, there seems to appear to be a very weak inverse linear relationship between the two variables.   
In addition, the correlation cor= `r cor(data$age, data$log_income)` between the two variables is low, indicating that there is only a weak relationship between them. 

### b. Create a linear regression model between log_income and age. What is the interpretation of the t-value? What kind of t-value would indicate a significant coefficient?  

```{r}
linearMod <- lm(log_income ~ age, data)
print(linearMod)
summary(linearMod)
```  

The t-value tests whether or not there is a statistically significant relationship between the dependent variable and the independent variable, that is whether or not the beta coefficient of the independent variable is significantly different from zero.

Mathematically, for a given beta coefficient (b), the t-test is computed as t = (b - 0)/SE(b), where SE(b) is the standard error of the coefficient b. The t-value measures the number of standard deviations that b is away from 0. The higher the t-value, the more significant independent variable. 

In our exercise, both the t-values for the intercept and age are highly significant, which means that there is a significant association between age and income. 

### c. What is the interpretation of the R-squared value?  What kind of R-squared value would indicate a good fit? 

The R-squared value is a goodness of fit measure. The R-squared ranges from 0 to 1 (i.e.: a number near 0 represents a regression that does not explain the variance in the dependent variable well and a number close to 1 does explain the observed variance in the dependent variable).
![Formula of R-squared](image1.png)

A high value of R-squared is a good indication.  
In our exercise, the R-squared we get is 0.01184. Or roughly 1.2% of the variance found in the dependent variable (income) can be explained by the independent variable (age).  

### d. What is the interpretation of the F-statistic?  What kind of F-statistic indicates a strong linear regression model?  

F-statistic is a good indicator of whether there is a relationship between our independent and the dependent variables. The further the F-statistic is from 1 the better it is. However, how much larger the F-statistic needs to be depends on both the number of data samples and the number of model parameters.  

![Formula of F-statistic](image.png)

The F-statistic is used to determine if the model is actually doing better than just guessing the mean value of y as the prediction (the "null model"). 

If the linear model is really just estimating the same as the null model, then the F-statistic should be about 1.  

A F-statistic that is much larger than 1 indicates a strong linear regression model.  

### e. View a detailed summary of the previous model.  What is the R-squared value?  Does this suggest that the model is a good fit? Why?
```{r}
summary(linearMod)
```  

Multiple R-squared:0.01184   

Adjusted R-squared: 0.01181 

This R-squared value is very far from 1 and near to 0 suggests that the model is not a good fit. 

### f. Create a scatter plot showing the effect education has on log_income.  Do you see any linear relationship between the two variables?  
```{r message=FALSE}
ggplot(data,aes(x= education, y=log_income)) +geom_point(alpha=0.2) +labs(x="Education",y="Log_Income",title="Scaterrplot Log_Income vs Education")
```  

This scatter plot seems to suggest that there is some sort of linear relationship between the two variables.The intercept seems to be positive.  

### g. Analyze a detailed summary of a linear regression model between log_income and education.  What is the R-squared value?  Is the model a good fit? Is it better than the previous model? 

```{r}
linearMod2 <- lm(log_income ~ education, data)
summary(linearMod2)
```  

Multiple R-squared: 0.5354

Adjusted R-squared: 0.5354

This R-squared value is much closer to 1 than our first model and suggests that the model is a decent fit. It is a better fit than the first model. 

### h. Analyze a detailed summary of a linear regression model between the dependent variable log_income, and the independent variables age, education, and employment.  Is this model a good fit?  Why?  What conclusions can be made about the different independent variables?
```{r}
linearMod3 <- lm(log_income ~ education + age + employment, data)
summary(linearMod3)
```   

This model appears to be a good, but not perfect, fit because the R-squared value is somewhat close to 1.  

The F-statistic is much larger than 1, and the p-value is extremely small, which indicates a strong model.  

The independent variable age seems to have the weakest linear relationship because its coefficient and t-value are small. 

### i. Based on the coefficients of the multiple regression model, by what percentage would income increase/decrease for every unit of education completed, while all other independent variables remained constant?  

For every unit of education completed, income increase 9.13%.  

### j. Create a graph that contains a y = x line and uses the multiple regression model to plot the predicted data points against the actual data points of the training set. 

```{r}
ggplot() + geom_point(aes(x= data$log_income, y=fitted(linearMod3)), alpha=0.5) + geom_line(aes(x=data$log_income, y= data$log_income), col = 'red') +labs(x="Actual income", y="Predicted income")
```  

### k. How well does the model predict across the various income ranges? 

In the graph, for lower incomes our model seems to over predict the income.  

For higher incomes, our model seems to slightly under predict the income.  

This graph indicates that our model provides reliable predictions around the median income range.  
